# Auction Web Analysis
* Utilize Kafka to gather daily change in log file
* Scrawl commodity data like name from an auction website and obtain visiting information like ip address through api.
* Use spark to find top 30 most frequently visited commodity every day
* Generate real time table to visulize all these data 
